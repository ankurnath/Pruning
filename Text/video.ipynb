{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GFLOW\n",
    "# pip install pytorchvideo==0.1.1\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "model_ckpt = \"MCG-NJU/videomae-base\" # pre-trained model from which to fine-tune\n",
    "batch_size = 8 # batch size for training and evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos: 405\n",
      "Unique classes: ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress'].\n"
     ]
    }
   ],
   "source": [
    "dataset_root_path = \"UCF101_subset\"\n",
    "import pathlib\n",
    "\n",
    "dataset_root_path = pathlib.Path(dataset_root_path)\n",
    "video_count_train = len(list(dataset_root_path.glob(\"train/*/*.avi\")))\n",
    "video_count_val = len(list(dataset_root_path.glob(\"val/*/*.avi\")))\n",
    "video_count_test = len(list(dataset_root_path.glob(\"test/*/*.avi\")))\n",
    "video_total = video_count_train + video_count_val + video_count_test\n",
    "print(f\"Total videos: {video_total}\")\n",
    "\n",
    "all_video_file_paths = (\n",
    "    list(dataset_root_path.glob(\"train/*/*.avi\"))\n",
    "    + list(dataset_root_path.glob(\"val/*/*.avi\"))\n",
    "    + list(dataset_root_path.glob(\"test/*/*.avi\"))\n",
    ")\n",
    "\n",
    "class_labels = sorted({str(path).split(\"/\")[2] for path in all_video_file_paths})\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"Unique classes: {list(label2id.keys())}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/a/anath/anaconda3/envs/gflow/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification,VideoMAEFeatureExtractor,VideoMAEModel\n",
    "\n",
    "\n",
    "image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\n",
    "model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorchvideo.data\n",
    "\n",
    "# import pytorchvideo.transforms.functional\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    height = width = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    height = image_processor.size[\"height\"]\n",
    "    width = image_processor.size[\"width\"]\n",
    "resize_to = (height, width)\n",
    "\n",
    "num_frames_to_sample = model.config.num_frames\n",
    "sample_rate = 4\n",
    "fps = 30\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    Resize(resize_to),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"train\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "\n",
    "val_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"val\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"test\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(model,dataset):\n",
    "\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    embeddings = []\n",
    "\n",
    "    for item in dataset:\n",
    "        video = item['video']\n",
    "        video = video.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        video = video.to(device)\n",
    "        with torch.no_grad():\n",
    "            embedding = model(video).last_hidden_state[:, 0].detach().cpu().squeeze().numpy()\n",
    "\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    embeddings = np.array(embeddings)\n",
    "    embeddings = torch.from_numpy(embeddings)\n",
    "    return embeddings\n",
    "all_candidate_embeddings =generate_embedding(model,val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(emb_one, emb_two):\n",
    "    \"\"\"Computes cosine similarity between two vectors.\"\"\"\n",
    "    scores = torch.nn.functional.cosine_similarity(emb_one, emb_two)\n",
    "    scores += 1\n",
    "    scores /=2\n",
    "    return scores.numpy().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "\n",
    "def show_images(images, cols=3, titles=None):\n",
    "    \"\"\"\n",
    "    Display a list of PIL images in a grid.\n",
    "    \n",
    "    Args:\n",
    "        images (list): A list of PIL Image objects to display.\n",
    "        cols (int): Number of columns in the grid (default is 3).\n",
    "        titles (list): Optional list of titles for each image (must match the number of images).\n",
    "    \"\"\"\n",
    "    num_images = len(images)\n",
    "    rows = ceil(num_images / cols)\n",
    "    \n",
    "    # Create subplots with dynamic number of rows and columns\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "\n",
    "    # Flatten the axes array for easy indexing, if needed\n",
    "    axes = axes.flatten() if num_images > 1 else [axes]\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < num_images:\n",
    "            ax.imshow(images[i])\n",
    "            ax.axis('off')  # Remove axis\n",
    "            if titles and i < len(titles):\n",
    "                ax.set_title(titles[i], fontsize=12)\n",
    "        else:\n",
    "            # Hide any unused subplot axes\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit\n",
    "\n",
    "\n",
    "# @njit(fastmath=True, parallel=True)\n",
    "def calculate_gains(candidate_similarity, query_similarity, solution_dense, size_solution, c=2):\n",
    "    \"\"\"\n",
    "    Calculate the gains for each candidate based on the current solution.\n",
    "\n",
    "    Args:\n",
    "    candidate_similarity (np.ndarray): A 2D array representing candidate-candidate similarity matrix.\n",
    "    query_similarity (np.ndarray): A 2D array representing candidate-query similarity matrix.\n",
    "    solution_dense (np.ndarray): Indices of selected candidates in the solution.\n",
    "    size_solution (int): Number of selected candidates in the solution.\n",
    "    c (float): Constant multiplier for query similarity.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: An array of gains for each candidate.\n",
    "    \"\"\"\n",
    "    num_candidates = len(candidate_similarity)\n",
    "    gains = np.zeros(num_candidates)\n",
    "\n",
    "    for candidate in range(num_candidates):\n",
    "        # print('candidate',candidate)\n",
    "        # print(np.sum(query_similarity[:,candidate]) *2)\n",
    "        # print(candidate_similarity[candidate, candidate])\n",
    "        gain = 10 * np.sum(query_similarity[:,candidate]) - candidate_similarity[candidate, candidate]\n",
    "\n",
    "        # print(gain)\n",
    "\n",
    "        # Adjust gain based on current solution\n",
    "        for selected_idx in range(size_solution):\n",
    "            selected_candidate = solution_dense[selected_idx]\n",
    "            gain -= 2 * candidate_similarity[candidate, selected_candidate]\n",
    "\n",
    "        gains[candidate] = gain\n",
    "\n",
    "    return gains\n",
    "\n",
    "# @njit(fastmath=True, parallel=True)\n",
    "def graph_cut(candidate_similarity, query_similarity, budget, ground_set):\n",
    "    \"\"\"\n",
    "    Perform the graph cut optimization to maximize similarity between candidates and the query.\n",
    "\n",
    "    Args:\n",
    "    candidate_similarity (np.ndarray): A 2D array representing candidate-candidate similarity matrix.\n",
    "    query_similarity (np.ndarray): A 2D array representing candidate-query similarity matrix.\n",
    "    budget (int): The number of candidates to select.\n",
    "    ground_set (list or np.ndarray): Set of candidate elements.\n",
    "\n",
    "    Returns:\n",
    "    obj_val (float): The final objective value achieved.\n",
    "    solution_dense (np.ndarray): Indices of selected candidates.\n",
    "    solution_sparse (np.ndarray): Sparse solution with binary representation (selected candidates are marked 1).\n",
    "    \"\"\"\n",
    "\n",
    "    num_candidates = len(candidate_similarity)\n",
    "\n",
    "    # Initialize variables\n",
    "    solution_sparse = np.zeros(num_candidates)\n",
    "    solution_dense = np.zeros(num_candidates, dtype=np.int32)\n",
    "    size_solution = 0\n",
    "    obj_val = 0.0\n",
    "    c = 2  # Constant multiplier for query similarity\n",
    "\n",
    "    # Loop over the budget to select candidates\n",
    "    for _ in range(budget):\n",
    "        # Calculate gains for all candidates\n",
    "        gains = calculate_gains(candidate_similarity, query_similarity, solution_dense, size_solution, c)\n",
    "\n",
    "        best_candidate = -1\n",
    "        max_gain = -float('inf')  # Initialize max_gain to negative infinity to ensure the first gain is selected\n",
    "        \n",
    "        for i in range(num_candidates):\n",
    "            if ground_set[i] == 1:  # Check if candidate is eligible (not already selected)\n",
    "                if gains[i] > max_gain:  # Find the candidate with the maximum gain\n",
    "                    max_gain = gains[i]\n",
    "                    best_candidate = i\n",
    "\n",
    "        if best_candidate == -1:\n",
    "            # No valid candidate was found, stop the loop\n",
    "            print(\"No valid candidate found, breaking the loop.\")\n",
    "            break\n",
    "\n",
    "        # Select the candidate with the highest gain\n",
    "        solution_dense[size_solution] = best_candidate\n",
    "        solution_sparse[best_candidate] = 1\n",
    "        size_solution += 1\n",
    "        obj_val += gains[best_candidate]\n",
    "\n",
    "\n",
    "    return obj_val, solution_dense[:size_solution], solution_sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_images=dataset[\"test\"][[0,50,100]][\"image\"]\n",
    "show_images(query_images) \n",
    "# Assuming `images` is a list or batch of images\n",
    "query_images_transformed_batch = torch.stack([transformation_chain(query_image) for query_image in query_images])\n",
    "query_images_transformed_batch = query_images_transformed_batch.to(device)\n",
    "# Prepare the batch dictionary\n",
    "new_batch = {\"pixel_values\": query_images_transformed_batch}\n",
    "# Compute the embeddings for all images in the batch\n",
    "with torch.no_grad():\n",
    "    query_embeddings = model(**new_batch).last_hidden_state[:, 0].cpu()\n",
    "\n",
    "\n",
    "candidate_similarity = np.array([compute_scores(all_candidate_embeddings, all_candidate_embeddings[i]) for i in range(all_candidate_embeddings.shape[0])]) \n",
    "query_similarity = np.array([compute_scores(all_candidate_embeddings, query_embeddings[i]) for i in range(query_embeddings.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def QS(candidate_similarity, query_similarity, delta=0.05, budget=5):\n",
    "    N = len(candidate_similarity)\n",
    "    print('Size of unpruned ground set', N)\n",
    "\n",
    "    # Current objective value\n",
    "    curr_obj = 0\n",
    "\n",
    "    # Ground set, dense and sparse solutions\n",
    "    ground_set = np.zeros(N)\n",
    "    solution_dense = np.zeros(N, dtype=np.int32)\n",
    "    solution_sparse = np.zeros(N)\n",
    "    size_solution = 0\n",
    "\n",
    "    for element in range(N):\n",
    "        gains = calculate_gains(candidate_similarity, query_similarity, solution_dense, size_solution, solution_sparse)\n",
    "\n",
    "        gain = gains[element]\n",
    "\n",
    "        if gain >= delta / budget * curr_obj:\n",
    "            curr_obj += gain\n",
    "            # Mark the element as selected in the ground set\n",
    "            ground_set[element] = 1\n",
    "            solution_sparse[element] = 1\n",
    "            solution_dense[size_solution] = element\n",
    "            size_solution += 1\n",
    "\n",
    "    print('Size of pruned ground set(QS)', ground_set.sum())\n",
    "    return ground_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import heapq\n",
    "\n",
    "def calculate_obj(candidate_similarity,query_similarity,solution):\n",
    "\n",
    "    obj_val = 0\n",
    "    # print(solution)\n",
    "    for candidate in solution:\n",
    "        obj_val += 10 * np.sum(query_similarity[:,candidate])\n",
    "\n",
    "\n",
    "    solution = list(solution)\n",
    "\n",
    "    for  i in range(len(solution)):\n",
    "        for j in range(i+1,len(solution)):\n",
    "            obj_val -= candidate_similarity[i,j]\n",
    "\n",
    "    return obj_val\n",
    "\n",
    "def SS(candidate_similarity,query_similarity,r=8,c=8):\n",
    "\n",
    "    n = len(candidate_similarity)\n",
    "    pruned_universe=set()\n",
    "\n",
    "    universe = list(range(n))\n",
    "\n",
    "    while len(universe)> r*np.log2(n):\n",
    "        U=random.sample(universe,int(r*np.log2(n)))\n",
    "        \n",
    "        universe = set(universe)\n",
    "\n",
    "        for element in tqdm(U):\n",
    "            universe.remove(element)\n",
    "\n",
    "        U = set(U)\n",
    "        pruned_universe=pruned_universe.union(U)\n",
    "        universe_gain=calculate_obj(candidate_similarity,query_similarity,universe) # f(V)\n",
    "\n",
    "        universe_u_gain = {} # f(V U u)\n",
    "        u_gain = {} # f(u)\n",
    "        # get all neighbors \n",
    "        \n",
    "        \n",
    "        for u in tqdm(U):\n",
    "            universe.add(u)\n",
    "            universe_u_gain[u] = calculate_obj (candidate_similarity,query_similarity ,universe)\n",
    "            universe.remove(u)\n",
    "            u_gain[u] = calculate_obj (candidate_similarity,query_similarity, [u])\n",
    "\n",
    "\n",
    "        lst = []\n",
    "\n",
    "        for v in tqdm(universe):\n",
    "\n",
    "            w=float('inf')\n",
    "            \n",
    "            # for u in graph.neighbors(v):\n",
    "                \n",
    "            for u in U:\n",
    "                # universe_copy=universe.copy()\n",
    "                # universe_copy.append(u)\n",
    "                \n",
    "                local_gain = calculate_obj(candidate_similarity,query_similarity,[u,v])-u_gain[u] # f(v U u) -f(u)\n",
    "                # print(local_gain)\n",
    "\n",
    "                global_gain = universe_u_gain[u]-universe_gain\n",
    "                w=min(w,local_gain-global_gain)\n",
    "\n",
    "            lst.append((w,v))\n",
    "\n",
    "        remove_nodes=heapq.nsmallest(int((1-1/np.sqrt(c))*len(universe)), lst)\n",
    "        # print(remove_nodes)\n",
    "        universe = set(universe)\n",
    "        for w,node in tqdm(remove_nodes):\n",
    "            # if w>0:\n",
    "            #     print(w)\n",
    "            universe.remove(node)\n",
    "            # universe.re\n",
    "        universe = list(universe)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    pruned_universe=pruned_universe.union(set(universe))\n",
    "\n",
    "    ground_set = np.zeros(n)\n",
    "    for element in pruned_universe:\n",
    "        ground_set[element] = 1\n",
    "\n",
    "    print('Size of pruned ground set(SS)', ground_set.sum())\n",
    "\n",
    "    return ground_set\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = defaultdict(list)\n",
    "delta = 0.05\n",
    "\n",
    "ground_set_SS = SS(candidate_similarity,query_similarity)\n",
    "for budget in [5,10,15,20]:\n",
    "\n",
    "    N = len(candidate_similarity)\n",
    "\n",
    "    ground_set_QS = QS(candidate_similarity,query_similarity,delta,budget)\n",
    "    retrived_images = np.where(ground_set_QS==1)[0]\n",
    "\n",
    "    obj_val_FS_QS=graph_cut(candidate_similarity=candidate_similarity,\n",
    "                            query_similarity=query_similarity,\n",
    "                            budget=budget,ground_set=ground_set_QS)[0]\n",
    "    \n",
    "    \n",
    "    obj_val_FS_SS=graph_cut(candidate_similarity=candidate_similarity,\n",
    "                            query_similarity=query_similarity,\n",
    "                            budget=budget,ground_set=ground_set_SS)[0]\n",
    "\n",
    "    \n",
    "\n",
    "    obj_val_FS=graph_cut(candidate_similarity=candidate_similarity,\n",
    "                            query_similarity=query_similarity,\n",
    "                            budget=budget,ground_set=np.ones(N))[0]\n",
    "    \n",
    "    num_repeat = 5\n",
    "    obj_val_FS_Random = 0\n",
    "    for i in range(num_repeat):\n",
    "        # Randomly select a mask\n",
    "        random_mask = np.random.choice(np.arange(N), size=int(ground_set_QS.sum()), replace=False)\n",
    "        \n",
    "        # Create the random ground set\n",
    "        random_ground_set = np.zeros(N)\n",
    "        random_ground_set[random_mask] = 1 \n",
    "        \n",
    "        # Calculate the objective value using graph_cut\n",
    "        obj_val_FS_Random += graph_cut(candidate_similarity=candidate_similarity,\n",
    "                                    query_similarity=query_similarity,\n",
    "                                    budget=budget,\n",
    "                                    ground_set=random_ground_set)[0]\n",
    "        \n",
    "    \n",
    "    obj_val_FS_Random /= num_repeat\n",
    "\n",
    "    # random_mask= np.random.choice(np.arange(N), size=int(ground_set.sum()), replace=False)\n",
    "    # random_ground_set = np.zeros(N)\n",
    "    # random_ground_set[random_mask] = 1 \n",
    "    # obj_val_FS_Random=graph_cut(candidate_similarity=candidate_similarity,\n",
    "    #                             query_similarity=query_similarity,\n",
    "    #                             budget=budget,ground_set=random_ground_set)[0]\n",
    "\n",
    "    # obj_val_FS_QS=facility_location(similarity,budget=budget,ground_set=ground_set)[0]\n",
    "    # obj_val_FS=facility_location(similarity,budget=budget,ground_set=np.ones(N))[0]\n",
    "\n",
    "    # random_mask= np.random.choice(np.arange(N), size=int(ground_set.sum()), replace=False)\n",
    "    # random_ground_set = np.zeros(N)\n",
    "    # random_ground_set[random_mask] = 1 \n",
    "    # obj_val_FS_Random=facility_location(similarity,budget=budget,ground_set=random_ground_set)[0]\n",
    "\n",
    "    df['Budget'].append(budget)\n",
    "    df['Obj(FS+QS)'].append(obj_val_FS_QS)\n",
    "    df['Obj(FS+SS)'].append(obj_val_FS_SS)\n",
    "    df['Obj(FS)'].append(obj_val_FS)\n",
    "    df['Obj(FS+Random)'].append(obj_val_FS_Random)\n",
    "\n",
    "    # show_images(candidate_subset[retrived_images][\"image\"])\n",
    "    # break\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the ratios for each row\n",
    "df['Ratio_Obj(FS+QS)_Obj(FS)'] = df['Obj(FS+QS)'] / df['Obj(FS)']\n",
    "df['Ratio_Obj(FS+SS)_Obj(FS)'] = df['Obj(FS+SS)'] / df['Obj(FS)']\n",
    "df['Ratio_Obj(FS+Random)_Obj(FS)'] = df['Obj(FS+Random)'] / df['Obj(FS)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(dpi=300)\n",
    "\n",
    "# Plotting the ratios with larger markers and improved styling\n",
    "sns.lineplot(data=df, x='Budget', y='Ratio_Obj(FS+QS)_Obj(FS)', label='QS$(P_g=2.86\\%)$', linestyle='--', marker='o', \n",
    "             markersize=8, color='#f46666')\n",
    "sns.lineplot(data=df, x='Budget', y='Ratio_Obj(FS+SS)_Obj(FS)', label='SS$(P_g=28.9\\%)$',linestyle='--' ,marker='s', \n",
    "             markersize=8, color='#37a1e2')\n",
    "sns.lineplot(data=df, x='Budget', y='Ratio_Obj(FS+Random)_Obj(FS)', label='Random$(P_g=2.86\\%)$',linestyle='--', marker='D', \n",
    "             markersize=8, color='#f67f10')\n",
    "\n",
    "# Labeling the plot\n",
    "# plt.title('Ratios of Different Objectives Relative to Obj(FS)')\n",
    "# plt.xlabel('Budget')\n",
    "# plt.ylabel('Ratio')\n",
    "# plt.legend(title='Objective', title_fontsize='13', fontsize='11')\n",
    "\n",
    "fontsize = 16\n",
    "\n",
    "# plt.title('Multi Budget results for Image retrival system \\n $P_g =$ percentage of the ground set')\n",
    "plt.title('Multi-Budget Analysis for Image Retrieval System\\n($P_g$: Pruned Ground Set in percentage)')\n",
    "\n",
    "plt.xlabel('Budget',fontsize=fontsize)\n",
    "plt.ylabel('Approximation ratio',fontsize=fontsize)\n",
    "\n",
    "plt.xticks(fontsize=fontsize )\n",
    "plt.yticks(fontsize=fontsize )\n",
    "\n",
    "# Adding a light grid\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.locator_params(nbins=6)\n",
    "# Legend\n",
    "plt.legend(fontsize='14')\n",
    "sns.despine()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcomb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
