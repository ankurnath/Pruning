{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/a/anath/anaconda3/envs/gflow/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# GFLOW\n",
    "# pip install pytorchvideo==0.1.1\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from numba import njit,prange\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from utils import save_to_pickle,load_from_pickle\n",
    "model_ckpt = \"MCG-NJU/videomae-base\" # pre-trained model from which to fine-tune\n",
    "batch_size = 8 # batch size for training and evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos: 0\n",
      "Unique classes: [].\n"
     ]
    }
   ],
   "source": [
    "dataset_root_path = \"UCF101_subset_whole\"\n",
    "import pathlib\n",
    "\n",
    "dataset_root_path = pathlib.Path(dataset_root_path)\n",
    "video_count_train = len(list(dataset_root_path.glob(\"train/*/*.avi\")))\n",
    "video_count_val = len(list(dataset_root_path.glob(\"val/*/*.avi\")))\n",
    "video_count_test = len(list(dataset_root_path.glob(\"test/*/*.avi\")))\n",
    "video_total = video_count_train + video_count_val + video_count_test\n",
    "print(f\"Total videos: {video_total}\")\n",
    "\n",
    "all_video_file_paths = (\n",
    "    list(dataset_root_path.glob(\"train/*/*.avi\"))\n",
    "    + list(dataset_root_path.glob(\"val/*/*.avi\"))\n",
    "    + list(dataset_root_path.glob(\"test/*/*.avi\"))\n",
    ")\n",
    "\n",
    "class_labels = sorted({str(path).split(\"/\")[2] for path in all_video_file_paths})\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"Unique classes: {list(label2id.keys())}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification,VideoMAEFeatureExtractor,VideoMAEModel\n",
    "\n",
    "\n",
    "image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\n",
    "model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "import pytorchvideo.data\n",
    "\n",
    "\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    Resize,\n",
    ")\n",
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    height = width = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    height = image_processor.size[\"height\"]\n",
    "    width = image_processor.size[\"width\"]\n",
    "resize_to = (height, width)\n",
    "\n",
    "num_frames_to_sample = model.config.num_frames\n",
    "sample_rate = 4\n",
    "fps = 30\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps\n",
    "\n",
    "transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    Resize(resize_to),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pytorchvideo.data.labeled_video_dataset(\n",
    "    data_path=os.path.join(dataset_root_path, \"train\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=transform,\n",
    "    video_sampler =torch.utils.data.SequentialSampler\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(model,dataset):\n",
    "\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    video_names = set()\n",
    "\n",
    "    file_names = []\n",
    "\n",
    "    # max_iter =16\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    # for item in tqdm(dataset):\n",
    "    for item in tqdm(dataset):\n",
    "        if item['video_name'] in video_names:\n",
    "            # print('clip_index',item['clip_index'])\n",
    "            continue\n",
    "        else:\n",
    "            video_names.add(item['video_name'])\n",
    "            video = item['video']\n",
    "            video = video.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "            video = video.to(device)\n",
    "            with torch.no_grad():\n",
    "                embedding = model(video).last_hidden_state[:, 0].detach().cpu().squeeze().numpy()\n",
    "\n",
    "            embeddings.append(embedding)\n",
    "            labels.append(item['label'])\n",
    "            file_names.append(item['video_name'])\n",
    "            cnt +=1 \n",
    "            if cnt == dataset.num_videos:\n",
    "                break\n",
    "\n",
    "    embeddings = np.array(embeddings)\n",
    "    embeddings = torch.from_numpy(embeddings)\n",
    "    return embeddings,labels,file_names\n",
    "# all_embeddings,labels,file_names = generate_embedding(model,dataset=dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations = dict(zip(file_names,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_to_pickle(data=annotations,file_path='annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(all_embeddings, 'all_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_812680/1199666853.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings=torch.load('all_embeddings.pt')\n"
     ]
    }
   ],
   "source": [
    "embeddings=torch.load('all_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been loaded from annotations\n"
     ]
    }
   ],
   "source": [
    "annotations = load_from_pickle('annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_list = list(annotations.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_with_labels =[[annotations_list[idx],embeddings[idx]] \n",
    "                         for idx in range(len(annotations_list))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(emb_one, emb_two):\n",
    "    \"\"\"Computes cosine similarity between two vectors.\"\"\"\n",
    "\n",
    "\n",
    "    emb_one_normalized = F.normalize(emb_one, p=2, dim=1).double()\n",
    "    emb_two_normalized = F.normalize(emb_two, p=2, dim=1).double()\n",
    "    scores= torch.mm(emb_one_normalized, emb_two_normalized.t())\n",
    "    scores += 1\n",
    "    scores /=2\n",
    "    return scores.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit,prange\n",
    "\n",
    "\n",
    "@njit(fastmath=True, parallel=True)\n",
    "def calculate_gains(candidate_similarity, query_similarity, \n",
    "                    solution_dense, size_solution, c=2):\n",
    "    \"\"\"\n",
    "    Calculate the gains for each candidate based on the current solution.\n",
    "\n",
    "    Args:\n",
    "    candidate_similarity (np.ndarray): A 2D array representing candidate-candidate similarity matrix.\n",
    "    query_similarity (np.ndarray): A 2D array representing candidate-query similarity matrix.\n",
    "    solution_dense (np.ndarray): Indices of selected candidates in the solution.\n",
    "    size_solution (int): Number of selected candidates in the solution.\n",
    "    c (float): Constant multiplier for query similarity.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: An array of gains for each candidate.\n",
    "    \"\"\"\n",
    "    num_candidates = len(candidate_similarity)\n",
    "    gains = np.zeros(num_candidates)\n",
    "\n",
    "    for candidate in prange(num_candidates):\n",
    "\n",
    "        gains[candidate] = 10 * np.sum(query_similarity[:,candidate]) - candidate_similarity[candidate, candidate]\n",
    "\n",
    "        # Adjust gain based on current solution\n",
    "        for selected_idx in range(size_solution):\n",
    "            selected_candidate = solution_dense[selected_idx]\n",
    "            gains[candidate] -= 2 * candidate_similarity[candidate, selected_candidate]\n",
    "        # gains[candidate] = gain\n",
    "\n",
    "    return gains\n",
    "\n",
    "# @njit(fastmath=True, parallel=True)\n",
    "def graph_cut(candidate_similarity, query_similarity,costs, budget, ground_set):\n",
    "    \"\"\"\n",
    "    Perform the graph cut optimization to maximize similarity between candidates and the query.\n",
    "\n",
    "    Args:\n",
    "    candidate_similarity (np.ndarray): A 2D array representing candidate-candidate similarity matrix.\n",
    "    query_similarity (np.ndarray): A 2D array representing candidate-query similarity matrix.\n",
    "    budget (int): The number of candidates to select.\n",
    "    ground_set (list or np.ndarray): Set of candidate elements.\n",
    "\n",
    "    Returns:\n",
    "    obj_val (float): The final objective value achieved.\n",
    "    solution_dense (np.ndarray): Indices of selected candidates.\n",
    "    solution_sparse (np.ndarray): Sparse solution with binary representation (selected candidates are marked 1).\n",
    "    \"\"\"\n",
    "\n",
    "    num_candidates = len(candidate_similarity)\n",
    "\n",
    "    # Initialize variables\n",
    "    solution_sparse = np.zeros(num_candidates)\n",
    "    solution_dense = np.zeros(num_candidates, dtype=np.int32)\n",
    "    size_solution = 0\n",
    "    obj_val = 0.0\n",
    "    c = 2  # Constant multiplier for query similarity\n",
    "\n",
    "    # Loop over the budget to select candidates\n",
    "    # for _ in range(budget):\n",
    "\n",
    "    constraint = 0\n",
    "    while True:\n",
    "        # Calculate gains for all candidates\n",
    "        gains = calculate_gains(candidate_similarity, query_similarity, \n",
    "                                solution_dense, size_solution, c)\n",
    "\n",
    "        best_candidate = -1\n",
    "        max_gain_ratio = 0 # Initialize max_gain to negative infinity to ensure the first gain is selected\n",
    "        \n",
    "        for i in range(num_candidates):\n",
    "            if ground_set[i] == 1 and solution_sparse[i] == 0:  # Check if candidate is eligible (not already selected)\n",
    "                if gains[i]/costs[i] > max_gain_ratio :  # Find the candidate with the maximum gain\n",
    "                    max_gain_ratio = gains[i]/costs[i]\n",
    "                    best_candidate = i\n",
    "\n",
    "        if best_candidate == -1:\n",
    "            # No valid candidate was found, stop the loop\n",
    "            print(\"No valid candidate found, breaking the loop.\")\n",
    "            break\n",
    "        \n",
    "        ground_set[best_candidate] = 0\n",
    "        # Select the candidate with the highest gain\n",
    "\n",
    "        if costs[best_candidate]+constraint <= budget:\n",
    "            solution_dense[size_solution] = best_candidate\n",
    "            solution_sparse[best_candidate] = 1\n",
    "            size_solution += 1\n",
    "            obj_val += gains[best_candidate]\n",
    "\n",
    "\n",
    "    return obj_val, solution_dense[:size_solution], solution_sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "@njit(fastmath=True, parallel=True)\n",
    "def QS(candidate_similarity, query_similarity,costs, delta=0.05, budget=5):\n",
    "    N = len(candidate_similarity)\n",
    "    print('Size of unpruned ground set', N)\n",
    "\n",
    "    # Current objective value\n",
    "    curr_obj = 0\n",
    "\n",
    "    # Ground set, dense and sparse solutions\n",
    "    ground_set = np.zeros(N)\n",
    "    solution_dense = np.zeros(N, dtype=np.int32)\n",
    "    solution_sparse = np.zeros(N)\n",
    "    size_solution = 0\n",
    "\n",
    "    for element in range(N):\n",
    "        gains = calculate_gains(candidate_similarity, query_similarity, \n",
    "                                solution_dense, size_solution, solution_sparse)\n",
    "\n",
    "        gain = gains[element]\n",
    "\n",
    "        if gain/costs[element] >= delta / budget * curr_obj:\n",
    "            curr_obj += gain\n",
    "            # Mark the element as selected in the ground set\n",
    "            ground_set[element] = 1\n",
    "            solution_sparse[element] = 1\n",
    "            solution_dense[size_solution] = element\n",
    "            size_solution += 1\n",
    "\n",
    "    print('Size of pruned ground set(QS)', ground_set.sum())\n",
    "    return ground_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(embeddings_with_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_candidate_embeddings_with_labels = embeddings_with_labels[:6000]\n",
    "all_query_embeddings_with_labels = embeddings_with_labels[6000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_candidate_embeddings= [item[1] for item in all_candidate_embeddings_with_labels]\n",
    "all_query_embeddings = [item[1] for item in all_query_embeddings_with_labels]\n",
    "\n",
    "all_candidate_labels= np.array([item[0] for item in all_candidate_embeddings_with_labels])\n",
    "all_query_labels = np.array([item[0] for item in all_query_embeddings_with_labels])\n",
    "all_candidate_embeddings = torch.stack(all_candidate_embeddings,dim=0)\n",
    "all_query_embeddings = torch.stack(all_query_embeddings,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[76 11 98]\n"
     ]
    }
   ],
   "source": [
    "queries_indices = [100,0,1]\n",
    "\n",
    "query_embeddings=all_query_embeddings[[queries_indices]]\n",
    "print(all_query_labels[queries_indices])\n",
    "\n",
    "\n",
    "candidate_similarity = compute_scores(all_candidate_embeddings,all_candidate_embeddings)\n",
    "query_similarity = compute_scores(query_embeddings,all_candidate_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import heapq\n",
    "\n",
    "def calculate_obj(candidate_similarity,query_similarity,solution):\n",
    "\n",
    "    obj_val = 0\n",
    "    # print(solution)\n",
    "    for candidate in solution:\n",
    "        obj_val += 10 * np.sum(query_similarity[:,candidate])\n",
    "\n",
    "\n",
    "    solution = list(solution)\n",
    "\n",
    "    for  i in range(len(solution)):\n",
    "        for j in range(i+1,len(solution)):\n",
    "            obj_val -= candidate_similarity[i,j]\n",
    "\n",
    "    return obj_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of unpruned ground set 6000\n",
      "Size of pruned ground set(QS) 16.0\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "Size of unpruned ground set 6000\n",
      "Size of pruned ground set(QS) 16.0\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "Size of unpruned ground set 6000\n",
      "Size of pruned ground set(QS) 16.0\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "Size of unpruned ground set 6000\n",
      "Size of pruned ground set(QS) 16.0\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "No valid candidate found, breaking the loop.\n",
      "   Budget  Ratio_Obj(QS)  Ratio_Obj(top-k)  Pg(QS)     Obj(FS)  \\\n",
      "0       5        0.95561          0.983249  0.2667  200.962084   \n",
      "1      10        0.95561          0.983249  0.2667  200.962084   \n",
      "2      15        0.95561          0.983249  0.2667  200.962084   \n",
      "3      20        0.95561          0.983249  0.2667  200.962084   \n",
      "\n",
      "   Ratio_Obj(Random)  \n",
      "0           0.117826  \n",
      "1           0.118495  \n",
      "2           0.119172  \n",
      "3           0.119111  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = defaultdict(list)\n",
    "delta = 0.001\n",
    "\n",
    "\n",
    "costs = np.ones(candidate_similarity.shape[0])\n",
    "# costs = np.random.uniform(0.9, 1, size=candidate_similarity.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "for budget in [5,10,15,20]:\n",
    "\n",
    "    N = len(candidate_similarity)\n",
    "\n",
    "    ground_set_QS = QS(candidate_similarity,query_similarity,costs,delta,budget)\n",
    "    # size_SS = round(np.sum(ground_set_SS)/len(ground_set_SS)*100,4)\n",
    "    size_QS = round(np.sum(ground_set_QS)/len(ground_set_QS)*100,4)\n",
    "    retrived_images = np.where(ground_set_QS==1)[0]\n",
    "\n",
    "    ### top-k\n",
    "\n",
    "    gains = calculate_gains(candidate_similarity=candidate_similarity,\n",
    "                            query_similarity=query_similarity,\n",
    "                            solution_dense=np.zeros(N, dtype=np.int32),\n",
    "                            size_solution=0)\n",
    "    \n",
    "    top_k_elements = np.argsort(gains)[::-1][:int(np.sum(ground_set_QS))]\n",
    "\n",
    "    ground_set_top_k = np.zeros(N)\n",
    "    for element in top_k_elements:\n",
    "        ground_set_top_k[int(element)] = 1\n",
    "\n",
    "    obj_val_top_k,solution_top_k,_=graph_cut(candidate_similarity=candidate_similarity,\n",
    "                            query_similarity=query_similarity,costs=costs,\n",
    "                            budget=budget,ground_set=ground_set_top_k)\n",
    "    \n",
    "\n",
    "\n",
    "    obj_val_FS_QS,solution_FS_QS,_=graph_cut(candidate_similarity=candidate_similarity,\n",
    "                            query_similarity=query_similarity,costs=costs,\n",
    "                            budget=budget,ground_set=ground_set_QS)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    obj_val_FS,solution_FS,_=graph_cut(candidate_similarity=candidate_similarity,\n",
    "                            query_similarity=query_similarity,costs=costs,\n",
    "                            budget=budget,ground_set=np.ones(N))\n",
    "    \n",
    "    num_repeat = 5\n",
    "    obj_val_FS_Random = 0\n",
    "    for i in range(num_repeat):\n",
    "        # Randomly select a mask\n",
    "        random_mask = np.random.choice(np.arange(N), \n",
    "                                       size=int(ground_set_QS.sum()), replace=False)\n",
    "        \n",
    "        # Create the random ground set\n",
    "        random_ground_set = np.zeros(N)\n",
    "        random_ground_set[random_mask] = 1 \n",
    "        \n",
    "        # Calculate the objective value using graph_cut\n",
    "        temp_obj_val_FS_Random,solution_FS_Random,_= graph_cut(candidate_similarity=candidate_similarity,\n",
    "                                    query_similarity=query_similarity,\n",
    "                                    budget=budget,costs=costs,\n",
    "                                    ground_set=random_ground_set)\n",
    "        \n",
    "        obj_val_FS_Random+=temp_obj_val_FS_Random\n",
    "\n",
    "        \n",
    "    \n",
    "    obj_val_FS_Random /= num_repeat\n",
    "\n",
    "    \n",
    "    df['Budget'].append(budget)\n",
    "    df['Ratio_Obj(QS)'].append(obj_val_FS_QS/obj_val_FS)\n",
    "    df['Ratio_Obj(top-k)'].append(obj_val_top_k/obj_val_FS)\n",
    "    # df['Ratio_Obj(FS+QS)'].append(obj_val_FS_QS)\n",
    "    df['Pg(QS)'].append(size_QS)\n",
    "    df['Obj(FS)'].append(obj_val_FS)\n",
    "    df['Ratio_Obj(Random)'].append(obj_val_FS_Random/obj_val_FS)\n",
    "    # df['Ratio_Obj(FS+Random)'].append(obj_val_FS_Random)\n",
    "\n",
    "    # break\n",
    "    \n",
    "    \n",
    "\n",
    "  \n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "df.to_pickle(f'video_UCF101')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_query_labels[queries_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_candidate_labels[solution_FS_Random]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_candidate_labels[solution_FS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_candidate_labels[solution_FS_QS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(dpi=300)\n",
    "\n",
    "# Plotting the ratios with larger markers and improved styling\n",
    "sns.lineplot(data=df, x='Budget', y='Ratio_Obj(FS+QS)', label='QS$(P_g=0.34\\%)$', linestyle='--', marker='o', \n",
    "             markersize=8, color='#f46666')\n",
    "# sns.lineplot(data=df, x='Budget', y='Ratio_Obj(FS+SS)', label='SS$(P_g=28.89\\%)$',linestyle='--' ,marker='s', \n",
    "#              markersize=8, color='#37a1e2')\n",
    "sns.lineplot(data=df, x='Budget', y='Ratio_Obj(FS+Random)', label='Random$(P_g=0.34\\%)$',linestyle='--', marker='D', \n",
    "             markersize=8, color='#f67f10')\n",
    "\n",
    "# Labeling the plot\n",
    "# plt.title('Ratios of Different Objectives Relative to Obj(FS)')\n",
    "# plt.xlabel('Budget')\n",
    "# plt.ylabel('Ratio')\n",
    "# plt.legend(title='Objective', title_fontsize='13', fontsize='11')\n",
    "\n",
    "fontsize = 16\n",
    "\n",
    "# plt.title('Multi Budget results for Image retrival system \\n $P_g =$ percentage of the ground set')\n",
    "plt.title('Multi-Budget Analysis for Image Retrieval System\\n($P_g$: Pruned Ground Set in percentage)')\n",
    "\n",
    "plt.xlabel('Budget',fontsize=fontsize)\n",
    "plt.ylabel('Approximation ratio',fontsize=fontsize)\n",
    "\n",
    "plt.xticks(fontsize=fontsize )\n",
    "plt.yticks(fontsize=fontsize )\n",
    "\n",
    "# Adding a light grid\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.locator_params(nbins=6)\n",
    "# Legend\n",
    "plt.legend(fontsize='14')\n",
    "sns.despine()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcomb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
