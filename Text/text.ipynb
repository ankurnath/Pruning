{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf51f16823b45d59bbcc70be71e92b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dadc8de1ff294fe59910cc2e3f91eb48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "from datasets import load_dataset\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "   return tokenizer(examples[\"text\"], truncation=True)\n",
    " \n",
    "# tokenized_train = imdb[\"train\"].shuffle(seed=42).map(preprocess_function, batched=True)\n",
    "shuffled_dataset = imdb[\"train\"].shuffle(seed=42)\n",
    "split_index = int(0.7 * len(shuffled_dataset))\n",
    "# Select the first 80% as the training set\n",
    "train_dataset = shuffled_dataset.select(range(split_index))\n",
    "# Select the remaining 20% as the validation set\n",
    "val_dataset = shuffled_dataset.select(range(split_index, len(shuffled_dataset)))\n",
    "# Split into training and validation sets (e.g., 80% train, 20% validation)\n",
    "# Tokenize both the training and validation datasets\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = imdb[\"test\"].shuffle(seed=42).map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "embeddings = model.encode(train_dataset['text'], convert_to_tensor=True)\n",
    "similarity = model.similarity(embeddings,embeddings)\n",
    "similarity =similarity.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of unpruned ground set 17500\n",
      "Size of pruned ground set 1355.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import njit,prange\n",
    "@njit(fastmath=True,parallel=True)\n",
    "def QS(similarity, costs, delta, budget):\n",
    "    \"\"\"\n",
    "    Maximizes the ground set based on similarity and cost constraints.\n",
    "\n",
    "    Args:\n",
    "        similarity (np.ndarray): NxN matrix representing pairwise similarity scores between text elements.\n",
    "        costs (np.ndarray): 1D array representing the cost associated with each text element.\n",
    "        delta (float): A constant to regulate the minimum gain to cost ratio.\n",
    "        budget (float): The available budget for selecting elements.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A binary array indicating which elements are selected in the ground set (1 if selected, 0 if not).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Number of elements in the set\n",
    "    N = len(similarity)\n",
    "    print('Size of unpruned ground set',N)\n",
    "    \n",
    "    # Current objective value\n",
    "    curr_obj = 0\n",
    "    \n",
    "    # Maximum similarity values for each element\n",
    "    max_similarity = np.zeros(N)\n",
    "    \n",
    "    # Ground set to keep track of selected elements (0: not selected, 1: selected)\n",
    "    ground_set = np.zeros(N)\n",
    "    \n",
    "    # Loop through all elements to consider them for the ground set\n",
    "    for element in range(N):\n",
    "        obj_val = 0\n",
    "        \n",
    "        # Calculate the objective value by updating the maximum similarity\n",
    "        for i in prange(N):\n",
    "            obj_val += max(max_similarity[i], similarity[i, element])\n",
    "\n",
    "        # Gain is the increase in the objective value\n",
    "        gain = obj_val - curr_obj\n",
    "        \n",
    "        # Check if the gain-to-cost ratio meets the threshold based on delta and budget\n",
    "        if gain / costs[element] >= delta / budget * curr_obj:\n",
    "            # Update the current objective value with the gain\n",
    "            curr_obj += gain\n",
    "            \n",
    "            # Mark the element as selected in the ground set\n",
    "            ground_set[element] = 1\n",
    "            for i in range(N):\n",
    "                max_similarity[i] = max(max_similarity[i], similarity[i, element])\n",
    "\n",
    "    print('Size of pruned ground set',ground_set.sum())\n",
    "    return ground_set\n",
    "\n",
    "costs = np.ones(len(similarity))\n",
    "delta = 0.1\n",
    "budget = 1000\n",
    "pruned_ground_set=QS(similarity, costs, delta, budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11270.472258493304\n",
      "1000.0\n",
      "11445.031223535538\n",
      "1000.0\n"
     ]
    }
   ],
   "source": [
    "from numba import njit,prange\n",
    "\n",
    "@njit(fastmath=True,parallel=True)\n",
    "def facility_location(similarity,costs,budget,ground_set):\n",
    "    # N= 25000\n",
    "    N = len(similarity)\n",
    "\n",
    "    max_obj = 0\n",
    "    total_cost = 0\n",
    "    solution_sparse = np.zeros(N)\n",
    "\n",
    "    max_similarity = np.zeros(N)\n",
    "\n",
    "    while total_cost < budget:\n",
    "\n",
    "        max_element = -1\n",
    "        obj_val = np.zeros(N)\n",
    "\n",
    "        for element in prange(N):\n",
    "            if solution_sparse[element] == 0 and ground_set[element] ==1 and costs[element]+total_cost <=budget:\n",
    "\n",
    "\n",
    "                for i in range(N):\n",
    "                    obj_val[element] += max(max_similarity[i],similarity[i,element])\n",
    "         \n",
    "\n",
    "        max_element = np.argmax(obj_val)\n",
    "\n",
    "        if obj_val[max_element] == max_obj:\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            solution_sparse[max_element] = 1\n",
    "            total_cost += costs[max_element]\n",
    "            for i in range(N):\n",
    "                max_similarity[i] = max(max_similarity[i],similarity[i,max_element])\n",
    "            \n",
    "            max_obj = obj_val[max_element]\n",
    "\n",
    "    print(max_obj)\n",
    "    print(solution_sparse.sum())\n",
    "    return max_obj,solution_sparse\n",
    "\n",
    "\n",
    "N = len(similarity)\n",
    "costs = np.ones(N)\n",
    "budget = 1000\n",
    "obj_val_pruned,solution_pruned = facility_location(costs=costs,budget=budget,similarity=similarity,ground_set=pruned_ground_set)\n",
    "        \n",
    "\n",
    "mask_pruned = np.where(solution_pruned==1)[0]\n",
    "obj_val_unpruned,solution_unpruned = facility_location(costs=costs,budget=budget,similarity=similarity,ground_set=np.ones(N))\n",
    "\n",
    "mask_unpruned= np.where(solution_unpruned ==1)[0] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 17500\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe9be2117fc49cc8cf0d20130bb9005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd47f18ac85405f929e171cd3c986fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44051888585090637, 'eval_accuracy': 0.8170666666666667, 'eval_runtime': 63.7897, 'eval_samples_per_second': 117.574, 'eval_steps_per_second': 7.352, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a4827ed9a146f8abada4a8d68ff16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34956881403923035, 'eval_accuracy': 0.8641333333333333, 'eval_runtime': 66.7428, 'eval_samples_per_second': 112.372, 'eval_steps_per_second': 7.027, 'epoch': 2.0}\n",
      "{'train_runtime': 184.442, 'train_samples_per_second': 10.844, 'train_steps_per_second': 0.683, 'train_loss': 0.4089543176075769, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=126, training_loss=0.4089543176075769, metrics={'train_runtime': 184.442, 'train_samples_per_second': 10.844, 'train_steps_per_second': 0.683, 'total_flos': 260935937715072.0, 'train_loss': 0.4089543176075769, 'epoch': 2.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"text_classification\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    # push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train.select(mask_pruned),\n",
    "    eval_dataset= tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.eval_dataset = test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18b5694d23745c487896c1d6a337fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.86104"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()['eval_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x7ffa1c8db640>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"text_classification\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=25,\n",
    "    per_device_eval_batch_size=25,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    # push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_from_pickle(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a pickle file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: The path to the pickle file.\n",
    "\n",
    "    Returns:\n",
    "    - loaded_data: The loaded data.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        loaded_data = pickle.load(file)\n",
    "    print(f'Data has been loaded from {file_path}')\n",
    "    return loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been loaded from IMDB\n"
     ]
    }
   ],
   "source": [
    "df = load_from_pickle('IMDB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ratio</th>\n",
       "      <th>Budget</th>\n",
       "      <th>Training Set Size</th>\n",
       "      <th>Pruned Ground Set Size (FS from QS)</th>\n",
       "      <th>Accuracy (FS + QS)</th>\n",
       "      <th>Pruned Training Set Size (FS)</th>\n",
       "      <th>Accuracy (FS)</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>175.0</td>\n",
       "      <td>17500</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>175</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ratio  Budget  Training Set Size  Pruned Ground Set Size (FS from QS)  \\\n",
       "0   0.01   175.0              17500                                224.0   \n",
       "\n",
       "   Accuracy (FS + QS)  Pruned Training Set Size (FS)  Accuracy (FS)  Accuracy  \n",
       "0                 0.5                            175            0.5    0.9142  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   Ratio  Budget  Training Set Size  ...  Accuracy (FS + QS)  Pruned Training Set Size (FS)  Accuracy (FS)\n",
    "0   0.01   175.0              17500  ...             0.60652                            175        0.55408\n",
    "1   0.05   875.0              17500  ...             0.86124                            875        0.86124\n",
    "2   0.10  1750.0              17500  ...             0.87268                           1750        0.87268"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcomb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
