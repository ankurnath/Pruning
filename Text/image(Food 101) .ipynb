{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/a/anath/anaconda3/envs/gcomb/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at nateraw/vit-base-food101 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "model_ckpt = 'nateraw/vit-base-food101'\n",
    "extractor = AutoFeatureExtractor.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)\n",
    "hidden_dim = model.config.hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75750it [00:00, 5734680.94it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"nateraw/food101\")\n",
    "seed = 42\n",
    "\n",
    "samples = []\n",
    "max_count = 100\n",
    "\n",
    "# # Create a dictionary to keep count of samples per label\n",
    "label_counts = defaultdict(int)\n",
    "\n",
    "# # Select samples\n",
    "for idx, label in tqdm(enumerate(dataset[\"train\"]['label'])):\n",
    "\n",
    "    # Only add sample if count for this label is less than max_count\n",
    "    if label_counts[label] < max_count:\n",
    "        samples.append(idx)\n",
    "        label_counts[label] += 1\n",
    "candidate_subset = dataset[\"train\"].select(samples).shuffle(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 10100\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae36e69b81a0405e84c422b1e83df677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "# Data transformation chain.\n",
    "transformation_chain = T.Compose(\n",
    "    [\n",
    "        # We first resize the input image to 256x256 and then we take center crop.\n",
    "        T.Resize(int((256 / 224) * extractor.size[\"height\"])),\n",
    "        T.CenterCrop(extractor.size[\"height\"]),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=extractor.image_mean, std=extractor.image_std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def extract_embeddings(model: torch.nn.Module):\n",
    "    \"\"\"Utility to compute embeddings.\"\"\"\n",
    "    device = model.device\n",
    "\n",
    "    def pp(batch):\n",
    "        images = batch[\"image\"]\n",
    "        image_batch_transformed = torch.stack(\n",
    "            [transformation_chain(Image.open(image).convert(\"RGB\")) for image in images]\n",
    "        )\n",
    "        new_batch = {\"pixel_values\": image_batch_transformed.to(device)}\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(**new_batch).last_hidden_state[:, 0].cpu()\n",
    "        return {\"embeddings\": embeddings}\n",
    "\n",
    "    return pp\n",
    "\n",
    "\n",
    "# Here, we map embedding extraction utility on our subset of candidate images.\n",
    "batch_size = 24\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "extract_fn = extract_embeddings(model.to(device))\n",
    "candidate_subset_emb = candidate_subset.map(extract_fn, batched=True, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_candidate_embeddings = np.array(candidate_subset_emb[\"embeddings\"])\n",
    "all_candidate_embeddings = torch.from_numpy(all_candidate_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_scores(emb_one, emb_two):\n",
    "#     \"\"\"Computes cosine similarity between two vectors.\"\"\"\n",
    "#     scores = torch.nn.functional.cosine_similarity(emb_one, emb_two)\n",
    "#     scores += 1\n",
    "#     scores /=2\n",
    "#     return scores.numpy().tolist()\n",
    "\n",
    "def compute_scores(emb_one, emb_two):\n",
    "    \"\"\"Computes cosine similarity between two vectors.\"\"\"\n",
    "\n",
    "\n",
    "    emb_one_normalized = F.normalize(emb_one, p=2, dim=1).double()\n",
    "    emb_two_normalized = F.normalize(emb_two, p=2, dim=1).double()\n",
    "    scores= torch.mm(emb_one_normalized, emb_two_normalized.t())\n",
    "    scores += 1\n",
    "    scores /=2\n",
    "    return scores.numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "\n",
    "def show_images(images, cols=5, titles=None):\n",
    "    \"\"\"\n",
    "    Display a list of PIL images in a grid.\n",
    "    \n",
    "    Args:\n",
    "        images (list): A list of PIL Image objects to display.\n",
    "        cols (int): Number of columns in the grid (default is 3).\n",
    "        titles (list): Optional list of titles for each image (must match the number of images).\n",
    "    \"\"\"\n",
    "    num_images = len(images)\n",
    "    rows = ceil(num_images / cols)\n",
    "    \n",
    "    # Create subplots with dynamic number of rows and columns\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(50, 10 * rows))\n",
    "    if titles:\n",
    "        plt.suptitle(titles,fontsize = 40)\n",
    "    # Flatten the axes array for easy indexing, if needed\n",
    "    axes = axes.flatten() if num_images > 1 else [axes]\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < num_images:\n",
    "            ax.imshow(images[i])\n",
    "            ax.axis('off')  # Remove axis\n",
    "            # if titles and i < len(titles):\n",
    "            #     ax.set_title(titles[i], fontsize=12)\n",
    "        else:\n",
    "            # Hide any unused subplot axes\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # @njit(fastmath=True,parallel=True)\n",
    "# def facility_location(similarity,budget,ground_set):\n",
    "#     # N = len(similarity)\n",
    "#     N = similarity.shape[1]\n",
    "#     candidate_set_size = similarity.shape[0]\n",
    "#     max_obj = 0\n",
    "#     total_cost = 0\n",
    "#     solution_sparse = np.zeros(N)\n",
    "#     max_similarity = np.zeros(candidate_set_size)\n",
    "\n",
    "#     for _ in range(budget):\n",
    "#         # max_element = -1\n",
    "#         # obj_val = np.zeros(N)\n",
    "#         gains = np.zeros(N)\n",
    "\n",
    "#         for element in range(N):\n",
    "#             if solution_sparse[element] == 0 and ground_set[element] == 1:\n",
    "\n",
    "#                 for candidate in range(candidate_set_size):\n",
    "#                     # obj_val[element] += max(max_similarity[candidate],similarity[candidate,element])\n",
    "#                     gains[element] += similarity[candidate,element]\n",
    "\n",
    "\n",
    "#         # if np.max(obj_val) == max_obj:\n",
    "#         if np.max(gains) == 0:\n",
    "\n",
    "#             break\n",
    "#         else:\n",
    "#             max_element = np.argmax(obj_val)\n",
    "#             solution_sparse[max_element] = 1\n",
    "#             # total_cost += costs[max_element]\n",
    "#             for i in range(candidate_set_size):\n",
    "#                 max_similarity[i] = max(max_similarity[i],similarity[i,max_element])\n",
    "            \n",
    "#         max_obj = obj_val[max_element]\n",
    "#     return max_obj,solution_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit,prange\n",
    "\n",
    "\n",
    "@njit(fastmath=True, parallel=True)\n",
    "def calculate_gains(candidate_similarity, query_similarity, solution_dense, size_solution, c=2):\n",
    "    \"\"\"\n",
    "    Calculate the gains for each candidate based on the current solution.\n",
    "\n",
    "    Args:\n",
    "    candidate_similarity (np.ndarray): A 2D array representing candidate-candidate similarity matrix.\n",
    "    query_similarity (np.ndarray): A 2D array representing candidate-query similarity matrix.\n",
    "    solution_dense (np.ndarray): Indices of selected candidates in the solution.\n",
    "    size_solution (int): Number of selected candidates in the solution.\n",
    "    c (float): Constant multiplier for query similarity.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: An array of gains for each candidate.\n",
    "    \"\"\"\n",
    "    num_candidates = len(candidate_similarity)\n",
    "    gains = np.zeros(num_candidates)\n",
    "\n",
    "    for candidate in prange(num_candidates):\n",
    "        # print('candidate',candidate)\n",
    "        # print(np.sum(query_similarity[:,candidate]) *2)\n",
    "        # print(candidate_similarity[candidate, candidate])\n",
    "        gains[candidate] = 10 * np.sum(query_similarity[:,candidate]) - candidate_similarity[candidate, candidate]\n",
    "\n",
    "        # print(gain)\n",
    "\n",
    "        # Adjust gain based on current solution\n",
    "        for selected_idx in range(size_solution):\n",
    "            selected_candidate = solution_dense[selected_idx]\n",
    "            gains[candidate] -= 2 * candidate_similarity[candidate, selected_candidate]\n",
    "        # gains[candidate] = gain\n",
    "\n",
    "    return gains\n",
    "\n",
    "# @njit(fastmath=True, parallel=True)\n",
    "def graph_cut(candidate_similarity, query_similarity, budget, ground_set):\n",
    "    \"\"\"\n",
    "    Perform the graph cut optimization to maximize similarity between candidates and the query.\n",
    "\n",
    "    Args:\n",
    "    candidate_similarity (np.ndarray): A 2D array representing candidate-candidate similarity matrix.\n",
    "    query_similarity (np.ndarray): A 2D array representing candidate-query similarity matrix.\n",
    "    budget (int): The number of candidates to select.\n",
    "    ground_set (list or np.ndarray): Set of candidate elements.\n",
    "\n",
    "    Returns:\n",
    "    obj_val (float): The final objective value achieved.\n",
    "    solution_dense (np.ndarray): Indices of selected candidates.\n",
    "    solution_sparse (np.ndarray): Sparse solution with binary representation (selected candidates are marked 1).\n",
    "    \"\"\"\n",
    "\n",
    "    num_candidates = len(candidate_similarity)\n",
    "\n",
    "    # Initialize variables\n",
    "    solution_sparse = np.zeros(num_candidates)\n",
    "    solution_dense = np.zeros(num_candidates, dtype=np.int32)\n",
    "    size_solution = 0\n",
    "    obj_val = 0.0\n",
    "    c = 2  # Constant multiplier for query similarity\n",
    "\n",
    "    # Loop over the budget to select candidates\n",
    "    for _ in range(budget):\n",
    "        # Calculate gains for all candidates\n",
    "        gains = calculate_gains(candidate_similarity, query_similarity, solution_dense, size_solution, c)\n",
    "\n",
    "        best_candidate = -1\n",
    "        max_gain = -float('inf')  # Initialize max_gain to negative infinity to ensure the first gain is selected\n",
    "        \n",
    "        for i in range(num_candidates):\n",
    "            if ground_set[i] == 1:  # Check if candidate is eligible (not already selected)\n",
    "                if gains[i] > max_gain and solution_sparse[i] == 0:  # Find the candidate with the maximum gain\n",
    "                    max_gain = gains[i]\n",
    "                    best_candidate = i\n",
    "\n",
    "        if best_candidate == -1:\n",
    "            # No valid candidate was found, stop the loop\n",
    "            print(\"No valid candidate found, breaking the loop.\")\n",
    "            break\n",
    "\n",
    "        # Select the candidate with the highest gain\n",
    "        solution_dense[size_solution] = best_candidate\n",
    "        solution_sparse[best_candidate] = 1\n",
    "        size_solution += 1\n",
    "        obj_val += gains[best_candidate]\n",
    "\n",
    "\n",
    "    return obj_val, solution_dense[:size_solution], solution_sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_similarity = compute_scores(all_candidate_embeddings,all_candidate_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "@njit(fastmath=True, parallel=True)\n",
    "def QS(candidate_similarity, query_similarity, delta=0.05, budget=5):\n",
    "    N = len(candidate_similarity)\n",
    "    print('Size of unpruned ground set', N)\n",
    "\n",
    "    # Current objective value\n",
    "    curr_obj = 0\n",
    "\n",
    "    # Ground set, dense and sparse solutions\n",
    "    ground_set = np.zeros(N)\n",
    "    solution_dense = np.zeros(N, dtype=np.int32)\n",
    "    solution_sparse = np.zeros(N)\n",
    "    size_solution = 0\n",
    "\n",
    "    for element in range(N):\n",
    "        gains = calculate_gains(candidate_similarity, query_similarity, \n",
    "                                solution_dense, size_solution, solution_sparse)\n",
    "\n",
    "        gain = gains[element]\n",
    "\n",
    "        if gain >= delta / budget * curr_obj:\n",
    "            curr_obj += gain\n",
    "            # Mark the element as selected in the ground set\n",
    "            ground_set[element] = 1\n",
    "            solution_sparse[element] = 1\n",
    "            solution_dense[size_solution] = element\n",
    "            size_solution += 1\n",
    "\n",
    "    print('Size of pruned ground set(QS)', ground_set.sum())\n",
    "    return ground_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import heapq\n",
    "\n",
    "def calculate_obj(candidate_similarity,query_similarity,solution):\n",
    "\n",
    "    obj_val = 0\n",
    "    # print(solution)\n",
    "    for candidate in solution:\n",
    "        obj_val += 10 * np.sum(query_similarity[:,candidate])\n",
    "\n",
    "\n",
    "    solution = list(solution)\n",
    "\n",
    "    for  i in range(len(solution)):\n",
    "        for j in range(i+1,len(solution)):\n",
    "            obj_val -= candidate_similarity[i,j]\n",
    "\n",
    "    return obj_val\n",
    "\n",
    "def SS(candidate_similarity,query_similarity,r=8,c=8):\n",
    "\n",
    "    n = len(candidate_similarity)\n",
    "    pruned_universe=set()\n",
    "\n",
    "    universe = list(range(n))\n",
    "\n",
    "    while len(universe)> r*np.log2(n):\n",
    "        U=random.sample(universe,int(r*np.log2(n)))\n",
    "        \n",
    "        universe = set(universe)\n",
    "\n",
    "        for element in tqdm(U):\n",
    "            universe.remove(element)\n",
    "\n",
    "        U = set(U)\n",
    "        pruned_universe=pruned_universe.union(U)\n",
    "        universe_gain=calculate_obj(candidate_similarity,query_similarity,universe) # f(V)\n",
    "\n",
    "        universe_u_gain = {} # f(V U u)\n",
    "        u_gain = {} # f(u)\n",
    "        # get all neighbors \n",
    "        \n",
    "        \n",
    "        for u in tqdm(U):\n",
    "            universe.add(u)\n",
    "            universe_u_gain[u] = calculate_obj (candidate_similarity,query_similarity ,universe)\n",
    "            universe.remove(u)\n",
    "            u_gain[u] = calculate_obj (candidate_similarity,query_similarity, [u])\n",
    "\n",
    "\n",
    "        lst = []\n",
    "\n",
    "        for v in tqdm(universe):\n",
    "\n",
    "            w=float('inf')\n",
    "            \n",
    "            # for u in graph.neighbors(v):\n",
    "                \n",
    "            for u in U:\n",
    "                # universe_copy=universe.copy()\n",
    "                # universe_copy.append(u)\n",
    "                \n",
    "                local_gain = calculate_obj(candidate_similarity,query_similarity,[u,v])-u_gain[u] # f(v U u) -f(u)\n",
    "                # print(local_gain)\n",
    "\n",
    "                global_gain = universe_u_gain[u]-universe_gain\n",
    "                w=min(w,local_gain-global_gain)\n",
    "\n",
    "            lst.append((w,v))\n",
    "\n",
    "        remove_nodes=heapq.nsmallest(int((1-1/np.sqrt(c))*len(universe)), lst)\n",
    "        # print(remove_nodes)\n",
    "        universe = set(universe)\n",
    "        for w,node in tqdm(remove_nodes):\n",
    "            # if w>0:\n",
    "            #     print(w)\n",
    "            universe.remove(node)\n",
    "            # universe.re\n",
    "        universe = list(universe)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    pruned_universe=pruned_universe.union(set(universe))\n",
    "\n",
    "    ground_set = np.zeros(n)\n",
    "    for element in pruned_universe:\n",
    "        ground_set[element] = 1\n",
    "\n",
    "    print('Size of pruned ground set(SS)', ground_set.sum())\n",
    "\n",
    "    return ground_set\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "random.seed(42)\n",
    "# Labels you want to sample from (e.g., labels 0-9)\n",
    "labels_to_sample = list(range(101))\n",
    "\n",
    "\n",
    "# Dictionary to keep track of samples per label\n",
    "label_samples = {label: [] for label in labels_to_sample}\n",
    "\n",
    "# Loop over dataset and group indices by label\n",
    "for idx, label in enumerate(dataset[\"validation\"]['label']):\n",
    "    if label in labels_to_sample:\n",
    "        label_samples[label].append(idx)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of unpruned ground set 10100\n",
      "Size of pruned ground set(QS) 37.0\n",
      "Size of unpruned ground set 10100\n",
      "Size of pruned ground set(QS) 37.0\n",
      "Size of unpruned ground set 10100\n",
      "Size of pruned ground set(QS) 37.0\n",
      "Size of unpruned ground set 10100\n",
      "Size of pruned ground set(QS) 37.0\n",
      "   Budget  Ratio_Obj(FS+QS)  Pg(FS+QS)  Obj(FS)  Ratio_Obj(FS+Random)\n",
      "0       5          0.955338     0.3663      1.0              0.776715\n",
      "1      10          0.906989     0.3663      1.0              0.715965\n",
      "2      15          0.880801     0.3663      1.0              0.679999\n",
      "3      20          0.852416     0.3663      1.0              0.618469\n"
     ]
    }
   ],
   "source": [
    "# Initialize list for query images\n",
    "\n",
    "# For each label, sample 5 random indices\n",
    "# for label in [0,1,2,3,4]:\n",
    "for label in [21]:\n",
    "    query_indices = []\n",
    "    sampled_indices = random.sample(label_samples[label],5)\n",
    "    query_indices.extend(sampled_indices)\n",
    "\n",
    "    # Select the images using the sampled indices\n",
    "    query_images = [ Image.open(image) for image in dataset[\"validation\"].select(query_indices)[\"image\"]]\n",
    "\n",
    "    # # query_images=dataset[\"test\"][[0,50,100,150,200]][\"img\"]\n",
    "    # show_images(query_images,titles='query_images') \n",
    "    # Assuming `images` is a list or batch of images\n",
    "    query_images_transformed_batch = torch.stack([transformation_chain(query_image) for query_image in query_images])\n",
    "    query_images_transformed_batch = query_images_transformed_batch.to(device)\n",
    "    # Prepare the batch dictionary\n",
    "    new_batch = {\"pixel_values\": query_images_transformed_batch}\n",
    "    # Compute the embeddings for all images in the batch\n",
    "    with torch.no_grad():\n",
    "        query_embeddings = model(**new_batch).last_hidden_state[:, 0].cpu()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    query_similarity = compute_scores(query_embeddings,all_candidate_embeddings)\n",
    "    # candidate_similarity = np.array([compute_scores(all_candidate_embeddings, all_candidate_embeddings[i]) for i in range(all_candidate_embeddings.shape[0])]) \n",
    "    # query_similarity = np.array([compute_scores(all_candidate_embeddings, query_embeddings[i]) for i in range(query_embeddings.shape[0])])\n",
    "\n",
    "    df = defaultdict(list)\n",
    "    delta = 0.01\n",
    "\n",
    "    ground_set_SS = SS(candidate_similarity,query_similarity)\n",
    "\n",
    "    size_SS = round(np.sum(ground_set_SS)/len(ground_set_SS)*100,4)\n",
    "    for budget in [5,10,15,20]:\n",
    "\n",
    "    # for budget in [5]:\n",
    "\n",
    "        N = len(candidate_similarity)\n",
    "\n",
    "        ground_set_QS = QS(candidate_similarity,query_similarity,delta,budget)\n",
    "        # size_SS = round(np.sum(ground_set_SS)/len(ground_set_SS)*100,4)\n",
    "        size_QS = round(np.sum(ground_set_QS)/len(ground_set_QS)*100,4)\n",
    "        retrived_images = np.where(ground_set_QS==1)[0]\n",
    "\n",
    "        obj_val_FS_QS,solution_FS_QS,_=graph_cut(candidate_similarity=candidate_similarity,\n",
    "                                query_similarity=query_similarity,\n",
    "                                budget=budget,ground_set=ground_set_QS)\n",
    "        \n",
    "        \n",
    "        # obj_val_FS_SS=graph_cut(candidate_similarity=candidate_similarity,\n",
    "        #                         query_similarity=query_similarity,\n",
    "        #                         budget=budget,ground_set=ground_set_SS)[0]\n",
    "\n",
    "        \n",
    "\n",
    "        obj_val_FS,solution_FS,_=graph_cut(candidate_similarity=candidate_similarity,\n",
    "                                query_similarity=query_similarity,\n",
    "                                budget=budget,ground_set=np.ones(N))\n",
    "        \n",
    "        num_repeat = 5\n",
    "        obj_val_FS_Random = 0\n",
    "        for i in range(num_repeat):\n",
    "            # Randomly select a mask\n",
    "            random_mask = np.random.choice(np.arange(N), size=int(ground_set_QS.sum()), replace=False)\n",
    "            \n",
    "            # Create the random ground set\n",
    "            random_ground_set = np.zeros(N)\n",
    "            random_ground_set[random_mask] = 1 \n",
    "            \n",
    "            # Calculate the objective value using graph_cut\n",
    "            temp_obj_val_FS_Random,solution_FS_Random,_= graph_cut(candidate_similarity=candidate_similarity,\n",
    "                                        query_similarity=query_similarity,\n",
    "                                        budget=budget,\n",
    "                                        ground_set=random_ground_set)\n",
    "            \n",
    "            obj_val_FS_Random+=temp_obj_val_FS_Random\n",
    "\n",
    "            \n",
    "        \n",
    "        obj_val_FS_Random /= num_repeat\n",
    "\n",
    "\n",
    "        df['Budget'].append(budget)\n",
    "        df['Ratio_Obj(FS+QS)'].append(obj_val_FS_QS/obj_val_FS)\n",
    "        df['Pg(FS+QS)'].append(size_QS)\n",
    "        # df['Ratio_Obj(FS+SS)'].append(obj_val_FS_SS/obj_val_FS)\n",
    "        # df['Pg(FS+SS)'].append(size_SS)\n",
    "        df['Obj(FS)'].append(obj_val_FS/obj_val_FS)\n",
    "        df['Ratio_Obj(FS+Random)'].append(obj_val_FS_Random/obj_val_FS)\n",
    "        \n",
    "        \n",
    "\n",
    "        # retrived_images = np.where(ground_set_QS==1)[0]\n",
    "        # show_images(candidate_subset[retrived_images][\"image\"])\n",
    "        # break\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(query_images,titles='query_images') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images([Image.open(image) for image in candidate_subset[solution_FS_QS][\"image\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images([Image.open(image) for image in candidate_subset[solution_FS][\"image\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images([Image.open(image) for image in candidate_subset[solution_FS_Random][\"image\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ground_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(dpi=300)\n",
    "\n",
    "# Plotting the ratios with larger markers and improved styling\n",
    "sns.lineplot(data=df, x='Budget', y='Ratio_Obj(FS+QS)', label='QS$(P_g=0.34\\%)$', linestyle='--', marker='o', \n",
    "             markersize=8, color='#f46666')\n",
    "# sns.lineplot(data=df, x='Budget', y='Ratio_Obj(FS+SS)', label='SS$(P_g=28.89\\%)$',linestyle='--' ,marker='s', \n",
    "#              markersize=8, color='#37a1e2')\n",
    "sns.lineplot(data=df, x='Budget', y='Ratio_Obj(FS+Random)', label='Random$(P_g=0.34\\%)$',linestyle='--', marker='D', \n",
    "             markersize=8, color='#f67f10')\n",
    "\n",
    "# Labeling the plot\n",
    "# plt.title('Ratios of Different Objectives Relative to Obj(FS)')\n",
    "# plt.xlabel('Budget')\n",
    "# plt.ylabel('Ratio')\n",
    "# plt.legend(title='Objective', title_fontsize='13', fontsize='11')\n",
    "\n",
    "fontsize = 16\n",
    "\n",
    "# plt.title('Multi Budget results for Image retrival system \\n $P_g =$ percentage of the ground set')\n",
    "plt.title('Multi-Budget Analysis for Image Retrieval System\\n($P_g$: Pruned Ground Set in percentage)')\n",
    "\n",
    "plt.xlabel('Budget',fontsize=fontsize)\n",
    "plt.ylabel('Approximation ratio',fontsize=fontsize)\n",
    "\n",
    "plt.xticks(fontsize=fontsize )\n",
    "plt.yticks(fontsize=fontsize )\n",
    "\n",
    "# Adding a light grid\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.locator_params(nbins=6)\n",
    "# Legend\n",
    "plt.legend(fontsize='14')\n",
    "sns.despine()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ground_set.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcomb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
